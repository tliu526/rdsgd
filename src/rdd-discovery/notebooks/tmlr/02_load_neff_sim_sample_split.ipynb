{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook for loading simulation results from updated neff experiments, with\n",
    "sample splitting per the TMLR review request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# user imports\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from utils.pwr import *\n",
    "from utils.sim import *\n",
    "from utils.rddd import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load simulated results data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_trials = 500\n",
    "fuzzy_gaps = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "alpha = 0.05\n",
    "\n",
    "low_cutoff = 0.25\n",
    "hi_cutoff = 0.75\n",
    "\n",
    "seeds = range(0, 401, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"/data/tliu/rdsgd/\"\n",
    "RESULTS_DIR = \"../../results/tmlr_rebuttal/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline discovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:10<00:00,  1.73s/it]\n"
     ]
    }
   ],
   "source": [
    "baseline_result_dict = {}\n",
    "\n",
    "for fuzzy_gap in tqdm(fuzzy_gaps):\n",
    "    error_dict = {\n",
    "        'x': {\n",
    "            'fp': 0,\n",
    "            'lower_fn': 0,\n",
    "            'upper_fn': 0\n",
    "        },\n",
    "\n",
    "        'covar': {\n",
    "            'fp': 0\n",
    "        },\n",
    "    }\n",
    "    pvals = {}\n",
    "    for seed in seeds:\n",
    "        with open(os.path.join(DATA_DIR, \"baseline\", f\"seed{seed}/blended_rdd_fixed_bw_{fuzzy_gap}.pkl\"), \"rb\") as f:\n",
    "            results = pickle.load(f)\n",
    "            for result in results:\n",
    "                x_thresholds = [np.round(c, decimals=2) for c in result['x'].keys()]\n",
    "                if low_cutoff not in x_thresholds:\n",
    "                    error_dict['x']['lower_fn'] += 1\n",
    "                else: \n",
    "                    x_thresholds.remove(0.25)\n",
    "\n",
    "                if hi_cutoff not in x_thresholds:\n",
    "                    error_dict['x']['upper_fn'] += 1\n",
    "                else:\n",
    "                    x_thresholds.remove(0.75)\n",
    "\n",
    "                error_dict['x']['fp'] += len(x_thresholds)\n",
    "                error_dict['covar']['fp'] += len(result['covar'].keys())\n",
    "\n",
    "            \n",
    "    baseline_result_dict[fuzzy_gap] = error_dict       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(baseline_result_dict, open(os.path.join(RESULTS_DIR, \"blend_baseline_results.dict\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Herlands et al. results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HERLANDS_RESULT_PATH = \"../../../herlands-lord3/results/\"\n",
    "\n",
    "herlands_dict = {}\n",
    "\n",
    "herlands_trial_dict = {}\n",
    "\n",
    "for gap in fuzzy_gaps:\n",
    "    gap_dict = {\n",
    "        'upper_tp': 0,\n",
    "        'lower_tp': 0,\n",
    "        'fp': 0,\n",
    "        'tot_tests': 0\n",
    "    }\n",
    "    trial_gap_dict = {\n",
    "        'lower': [],\n",
    "        'upper': []\n",
    "    }\n",
    "    for seed in range(0, 500, 100):\n",
    "        results = pickle.load(open(os.path.join(DATA_DIR, \"herlands-lord3\", \"seed{}/herlands_results_gap{}.pkl\".format(seed, gap)), \"rb\"))\n",
    "        gap_dict['lower_tp'] += sum([x[0] for x in results])\n",
    "        gap_dict['upper_tp'] += sum([x[1] for x in results])\n",
    "        gap_dict['tot_tests'] += sum([x[2] for x in results])\n",
    "\n",
    "        for lower, upper, *_ in results:\n",
    "            trial_gap_dict['lower'].append(int(lower))\n",
    "            trial_gap_dict['upper'].append(int(upper))\n",
    "        \n",
    "    herlands_dict[gap] = gap_dict\n",
    "    herlands_trial_dict[gap] = trial_gap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(herlands_dict, open(os.path.join(RESULTS_DIR, \"herlands_results.dict\"), \"wb\"), -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgroup discovery with sample splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = 0.25\n",
    "sample_size = 1000\n",
    "seed_offset = 2000\n",
    "\n",
    "def process_subgroup_tree_results_sample_split(fuzzy_gap, seeds):\n",
    "    \"\"\"Process subgroup tree results, with a holdout set\"\"\"\n",
    "    pvals = {\n",
    "        'trial': [],\n",
    "        'cutoff': [],\n",
    "        'tau_pval': [],\n",
    "        'neff_pval': [],\n",
    "        'rule_length': [],\n",
    "    }\n",
    "    for seed in seeds:\n",
    "        with open(os.path.join(DATA_DIR, \"subgroup\", f\"seed{seed}_blended_rdd_fixed_bw_{fuzzy_gap}.pkl\"), \"rb\") as f:\n",
    "            result, n_tests = pickle.load(f)    \n",
    "            \n",
    "            x_dict = result['x']\n",
    "            x_thresholds = [np.round(c, decimals=2) for c in x_dict.keys()]\n",
    "            \n",
    "            for x_cutoff in x_thresholds:\n",
    "                nodes = x_dict[x_cutoff]\n",
    "                \n",
    "                for node in nodes:\n",
    "                    if node['llr_results'] is None:\n",
    "                        continue\n",
    "                \n",
    "                    # generate an iid hold-out set\n",
    "                    holdout = generate_cont_blended_rdd(\n",
    "                        n=sample_size,\n",
    "                        # make sure we don't use the same seed as the training set\n",
    "                        seed=seed + seed_offset,\n",
    "                        fuzzy_gap=fuzzy_gap\n",
    "                    )\n",
    "\n",
    "                    # filter to bandwidth, which was fixed to 0.25\n",
    "                    holdout = holdout[(holdout['x'] >= x_cutoff - bw) & (holdout['x'] <= x_cutoff + bw)]\n",
    "\n",
    "                    # apply the rule path to the holdout set\n",
    "                    rule_path = node['rule_path']\n",
    "\n",
    "                    # omit the last rule, which is the terminal node\n",
    "                    for rule in rule_path[:-1]:\n",
    "                        if rule.path_dir == '<':\n",
    "                            holdout = holdout[holdout[rule.feature] < rule.threshold]\n",
    "                        elif rule.path_dir == '>=':\n",
    "                            holdout = holdout[holdout[rule.feature] >= rule.threshold]\n",
    "                        elif rule.path_dir == '<=':\n",
    "                            holdout = holdout[holdout[rule.feature] <= rule.threshold]\n",
    "                        elif rule.path_dir == '>':\n",
    "                            holdout = holdout[holdout[rule.feature] > rule.threshold]\n",
    "                        elif rule.path_dir == '==':\n",
    "                            holdout = holdout[holdout[rule.feature] == rule.threshold]\n",
    "                        \n",
    "                    # test the discovered subgroup on the holdout set\n",
    "                    llr_results, _, _ = test_discontinuity(holdout, x_cutoff, 'x', treat='t', bw=bw, kernel='triangular')\n",
    "\n",
    "                    pvals['trial'].append(seed)\n",
    "                    pvals['cutoff'].append(x_cutoff)\n",
    "                    pvals['neff_pval'].append(node['neff_pval'])\n",
    "                    pvals['tau_pval'].append(llr_results.pvalues['z'])\n",
    "                    pvals['rule_length'].append(len(node['rule_path']))\n",
    "                    #pvals[seed]['x_all'][x_cutoff].append((node['llr_results'].pvalues['z'], node['neff_pval'], len(node['rule_path'])))\n",
    "    \n",
    "    pval_df = pd.DataFrame.from_dict(pvals)\n",
    "    pval_df['fuzzy_gap'] = fuzzy_gap\n",
    "    return pval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "invalid value encountered in sqrt\n",
      "invalid value encountered in sqrt\n",
      "invalid value encountered in sqrt\n",
      "invalid value encountered in sqrt\n",
      "invalid value encountered in sqrt\n",
      "invalid value encountered in sqrt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 753 ms, sys: 352 ms, total: 1.11 s\n",
      "Wall time: 15min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#subgroup_results_dict = {}\n",
    "\n",
    "seeds = range(0, n_trials)\n",
    "\n",
    "f_args = [(fuzzy_gap, seeds) for fuzzy_gap in fuzzy_gaps]\n",
    "with multiprocessing.Pool(8) as p:\n",
    "    results = p.starmap(process_subgroup_tree_results_sample_split, f_args)\n",
    "\n",
    "#subgroup_results_dict = {fuzzy_gap: pvals for fuzzy_gap, pvals in results}\n",
    "pval_df = pd.concat(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pval_df.to_pickle(os.path.join(RESULTS_DIR, \"subgroup_neff_pval_sample_split.df\"))\n",
    "#pval_df = pd.read_pickle(os.path.join(RESULTS_DIR, \"subgroup_neff_pval_sample_split.df\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute corrected p-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:00<?, ?it/s]\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "100%|██████████| 6/6 [00:10<00:00,  1.76s/it]\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.05\n",
    "low_cutoff = 0.25\n",
    "upper_cutoff = 0.75\n",
    "\n",
    "#tp_all = {}\n",
    "\n",
    "nb_dict = {}\n",
    "\n",
    "subgroup_trial_dict = {}\n",
    "\n",
    "#pval_df = result_dict['x_all']\n",
    "#pval_df.shape\n",
    "for gap in tqdm(fuzzy_gaps):\n",
    "    gap_dict = {\n",
    "        'upper_tp': 0,\n",
    "        'lower_tp': 0,\n",
    "        'fp': 0,\n",
    "        'tot_tests': 0,\n",
    "        'tot_upper_tp': 0,\n",
    "        'tot_lower_tp': 0,\n",
    "        #'fp_cutoffs': set()\n",
    "    }\n",
    "    \n",
    "    for trial in range(n_trials):\n",
    "        trial_df = pval_df[(pval_df['trial'] == trial) & (pval_df['fuzzy_gap'] == gap)]\n",
    "        gap_dict['tot_tests'] += trial_df.shape[0]\n",
    "        \n",
    "        method = 'bonf'\n",
    "        #reject_neff, _ = pg.multicomp(list(trial_df['neff_pval']), method=method, alpha=alpha)\n",
    "        reject_neff = trial_df['neff_pval'] < alpha\n",
    "\n",
    "        test_dict = trial_df.groupby('cutoff')['trial'].count().to_dict()\n",
    "        trial_df['n_tests'] = trial_df['cutoff'].apply(lambda x: test_dict[x])\n",
    "        \n",
    "        #print(reject_neff.sum())\n",
    "        #print(trial_df['tau_pval'].shape)\n",
    "        \n",
    "        reject_z = trial_df['tau_pval'] < (alpha / trial_df['n_tests'])\n",
    "        \n",
    "        # original correction\n",
    "        reject_z, _ = pg.multicomp(list(trial_df['tau_pval']), method=method, alpha=alpha)\n",
    "\n",
    "        reject = (reject_z & reject_neff) | (reject_z & (trial_df['rule_length'] == 1))\n",
    "        if not hasattr(reject, '__iter__'):\n",
    "            reject = [reject]\n",
    "        sig_df = trial_df[reject]\n",
    "        \n",
    "        if low_cutoff in list(sig_df['cutoff']):\n",
    "            gap_dict['lower_tp'] += 1\n",
    "            gap_dict['tot_lower_tp'] += sig_df[sig_df['cutoff'] == low_cutoff].shape[0]\n",
    "\n",
    "        if upper_cutoff in list(sig_df['cutoff']):\n",
    "            gap_dict['upper_tp'] += 1\n",
    "            gap_dict['tot_upper_tp'] += sig_df[sig_df['cutoff'] == upper_cutoff].shape[0]\n",
    "            \n",
    "        # remaining sig values are false positives\n",
    "        fp_df = sig_df[~sig_df['cutoff'].isin([low_cutoff, upper_cutoff])]\n",
    "        #print(fp_df)\n",
    "        gap_dict['fp'] += fp_df.shape[0]\n",
    "        # for x in fp_df['cutoff'].unique():\n",
    "        #     gap_dict['fp_cutoffs'].add(x)\n",
    "        \n",
    "    nb_dict[gap] = gap_dict\n",
    "    #subgroup_trial_dict[gap] = trial_gap_dict\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(nb_dict, open(os.path.join(RESULTS_DIR, \"rdsgd_neff_results_sample_split.dict\"), \"wb\"), -1)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "rdd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c5e384680b37c811d5c050863cd2e1b7b7fb22fa41e6ef4def208bf90ddd8ddd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
